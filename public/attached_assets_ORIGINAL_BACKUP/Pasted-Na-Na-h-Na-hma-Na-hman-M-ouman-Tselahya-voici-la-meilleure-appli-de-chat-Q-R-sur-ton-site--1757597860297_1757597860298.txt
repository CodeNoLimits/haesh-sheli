Na Na’h Na’hma Na’hman Méouman.

Tselahya, voici **la meilleure appli de chat “Q\&R sur ton site”** que tu peux faire *tout de suite* sur Replit, avec une **recette fiable et précise** (RAG : Retrieval-Augmented Generation). Je te donne les technos exactes à demander + les étapes opérationnelles. Je cite les docs officielles aux points clés pour éviter toute hallucination.

---

# 1) Pile technique recommandée (prête pour Replit)

**But :** un chatbot qui répond à *n’importe quelle question sur HaEsh Sheli* à partir du contenu du site.

* **Front/Serveur :** Next.js (Node.js) avec **Vercel AI SDK** pour le *streaming* en live des réponses. ([Vercel][1])
* **Vector DB :** **Postgres + pgvector** (via Supabase ou Neon) pour stocker les embeddings et faire la recherche sémantique. ([Supabase][2])
* **Embeddings :** **OpenAI `text-embedding-3-large` (3072 dims)** pour un rappel de haute qualité, multilingue (hébreu compris). ([OpenAI][3])
* **LLM de réponse :** **OpenAI `gpt-4o-mini`** (rapide/éco) via l’API (streaming). ([OpenAI Platform][4])

> Pourquoi ce combo ?
>
> * **Vercel AI SDK** facilite le streaming SSE côté Next.js (moins de boilerplate). ([Vercel][1])
> * **pgvector** est le standard Postgres pour RAG, avec index HNSW/IVFFlat si besoin de perf. ([GitHub][5])
> * **text-embedding-3-large** offre le meilleur rappel (3072 dims) pour QA multilingue. ([OpenAI][3])

---

# 2) Ce que tu demandes **à Replit**, mot pour mot

> **“Crée un environnement Node.js (Next.js 14+). Ajoute les secrets `OPENAI_API_KEY` et `DATABASE_URL`.
> Le `DATABASE_URL` pointe vers un Postgres managé (Supabase/Neon) avec l’extension `pgvector` activée.
> Installe les packages `ai`, `openai`, `pg`, `drizzle-orm` (ou Prisma si tu préfères), `zod`.
> Je veux un endpoint `/api/chat` (streaming), un script d’ingestion qui *crawl* mes pages et les ancre en base (table `documents`, `chunks` + `embedding vector`).”**

Pour **pgvector côté DB**, active l’extension et la colonne vecteur selon la doc (ex. Supabase) : `create extension if not exists vector;` + colonnes `embedding vector(3072)`. ([Supabase][2])

---

# 3) Schéma minimal **sans ambiguïté**

**Tables (Postgres + pgvector)**

* `documents(id, url, title, content, created_at)`
* `chunks(id, document_id, content, embedding vector(3072))` (+ index HNSW/IVFFlat si volume). ([GitHub][5])

**Flux RAG**

1. **Ingestion** : on récupère les pages du site (sitemap / routes), on *nettoie* le HTML → texte, on découpe en *chunks* (500–1 000 tokens), on calcule les **embeddings** `text-embedding-3-large`, on insère dans `chunks`. ([OpenAI][3])
2. **Question** : on embed la requête, on cherche les `k` chunks pertinents (cosine), on compose un prompt “réponds en HE/FR, cite les URLs sources”, le **LLM** (`gpt-4o-mini`) génère la réponse **en streaming** vers le front. ([Vercel][1])

---

# 4) Étapes **opérationnelles** (copier-coller dans tes tâches)

## A. Préparer la base (Supabase/Neon)

1. Crée la DB Postgres managée.
2. Active **pgvector** :

   ```sql
   create extension if not exists vector;
   ```

   (Doc Supabase pgvector). ([Supabase][2])
3. Crée les tables :

   ```sql
   create table documents (
     id uuid primary key default gen_random_uuid(),
     url text unique,
     title text,
     content text,
     created_at timestamp default now()
   );

   create table chunks (
     id uuid primary key default gen_random_uuid(),
     document_id uuid references documents(id) on delete cascade,
     content text,
     embedding vector(3072)  -- taille alignée au modèle
   );

   -- Index ANN (optionnel haute-perf)
   -- create index on chunks using ivfflat (embedding vector_cosine_ops) with (lists = 100);
   -- ou HNSW : create index on chunks using hnsw (embedding vector_cosine_ops);
   ```

   (Index ANN disponibles dans pgvector). ([GitHub][5])

## B. Replit : projet Next.js + clés

1. Secrets :

   * `OPENAI_API_KEY=xxxx`
   * `DATABASE_URL=postgres://…`
2. Dépendances :

   ```bash
   npm i ai openai pg drizzle-orm zod
   # (ou Prisma si tu préfères)
   ```

## C. Script d’**ingestion** (server script)

* Rôle : **crawling** (sitemap/URLs connues), nettoyage HTML → texte, **chunking**, embeddings, insert DB.
* Embeddings **`text-embedding-3-large`** → 3072 dims. (OpenAI embeddings). ([OpenAI Platform][6])
* Tu peux déclencher le script à la main ou via CRON.

## D. Endpoint **/api/chat** (Next.js Route Handler)

1. Reçoit `{question, lang}`.
2. Calcule l’**embedding** de la question. (OpenAI Embeddings). ([OpenAI Platform][6])
3. **Recherche vectorielle** dans `chunks` par similarité (cosine). (pgvector). ([Supabase][2])
4. Compose un prompt *sobre* avec les meilleurs `k` extraits + leurs URLs.
5. Appelle le LLM **`gpt-4o-mini`** en **streaming** (Vercel AI SDK *ou* Responses API streaming). ([Vercel][1])
6. Retourne un flux texte + métadonnées (sources).

> **NB :** Le Vercel **AI SDK** fournit des helpers de streaming utilisables hors Vercel (sur Replit) car c’est une lib côté Node/Next. ([Vercel][1])

---

# 5) Gabarits de code (ultra-minimum, à adapter)

## a) Ingestion (pseudo-code clair)

```ts
// ingest.ts (node script)
import { OpenAI } from "openai";
import { Pool } from "pg";
import { chunkText } from "./chunker"; // simple util

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });
const db = new Pool({ connectionString: process.env.DATABASE_URL });

async function embed(text: string) {
  const r = await openai.embeddings.create({
    model: "text-embedding-3-large",
    input: text
  }); // 3072 dims
  return r.data[0].embedding;
}

async function upsertDoc(url: string, title: string, content: string) {
  const { rows } = await db.query(
    `insert into documents(url, title, content)
     values ($1,$2,$3)
     on conflict (url) do update set title=excluded.title, content=excluded.content
     returning id`,
    [url, title, content]
  );
  const docId = rows[0].id;

  const parts = chunkText(content, 800); // 600–1000 tokens
  for (const part of parts) {
    const vec = await embed(part);
    await db.query(
      `insert into chunks(document_id, content, embedding) values ($1, $2, $3)`,
      [docId, part, vec]
    );
  }
}
```

*(L’API embeddings OpenAI et les 3072 dimensions sont documentées ici.)* ([OpenAI Platform][6])

## b) Recherche + chat (route Next.js, streaming)

```ts
// app/api/chat/route.ts
import { NextRequest } from "next/server";
import { Pool } from "pg";
import { OpenAI } from "openai";
import { streamText } from "ai"; // Vercel AI SDK

const db = new Pool({ connectionString: process.env.DATABASE_URL });
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

export async function POST(req: NextRequest) {
  const { question, lang = "he" } = await req.json();

  // 1) Embedding de la question
  const qryEmb = await openai.embeddings.create({
    model: "text-embedding-3-large",
    input: question
  });

  // 2) Recherche vectorielle pgvector (cosine)
  const { rows: ctx } = await db.query(
    `select c.content, d.url
     from chunks c
     join documents d on d.id = c.document_id
     order by c.embedding <=> $1
     limit 8`,
    [qryEmb.data[0].embedding] // opérateur distance pgvector
  );

  // 3) Contexte
  const sources = ctx.map((r: any) => `- ${r.url}`).join("\n");
  const context = ctx.map((r: any) => r.content).join("\n---\n");

  // 4) Streaming de la réponse (LLM rapide)
  const stream = await streamText({
    model: "openai:gpt-4o-mini",
    system: `Tu réponds en ${lang}. Appuie-toi STRICTEMENT sur le contexte fourni. 
             Si l'info manque, dis-le et propose la page la plus proche.`,
    prompt: `Question:\n${question}\n\nContexte:\n${context}\n\nCite les sources:\n${sources}`
  });

  return stream.toAIStreamResponse(); // flux SSE
}
```

*(Streaming côté Next + AI SDK, conforme aux docs.)* ([Vercel][1])

---

# 6) Intégration Front (composant “Chat”)

* Un simple composant React qui POST sur `/api/chat`, lit le **flux** et affiche ligne par ligne (le **Vercel AI SDK** fournit aussi des helpers UI prêts à l’emploi). ([sdk.vercel.ai][7])

---

# 7) Bonnes pratiques **anti-hallucination**

1. **Toujours** récupérer 6–10 chunks et **citer les URLs**.
2. **Message système strict** : “Réponds uniquement depuis le contexte; sinon, dis que l’info n’est pas dans le site.”
3. **Filtres** : enlever menus/pieds de page lors de l’ingestion pour un contexte propre.
4. **Langues** : créer des embeddings sur le *texte réel* (HE/FR/EN), ça gère le multilingue. (Embeddings OpenAI multi-langues). ([OpenAI Platform][6])
5. **Mises à jour auto** : re-ingérer via CRON à chaque ajout/MAJ contenu (Supabase “Automatic embeddings” si tu veux déléguer le pipeline). ([Supabase][8])

---

# 8) Variante “SaaS simple” (si tu veux juste brancher un widget)

* Tu peux aussi coller un widget de chat humain (Crisp/Intercom) **et** router leurs webhooks vers *ton* endpoint RAG `/api/chat` pour des réponses IA + fallback humain. *(Sans doc officielle ici pour éviter le hors-sujet; la solution RAG ci-dessus reste la base solide.)*

---

# 9) Check-list finale (copier dans Replit)

* [ ] Secrets `OPENAI_API_KEY`, `DATABASE_URL`
* [ ] DB Postgres avec **pgvector** activé (extension) ([Supabase][2])
* [ ] Tables `documents`, `chunks(embedding vector(3072))`
* [ ] Script `ingest.ts` (crawl → chunk → embed → insert)
* [ ] Route `/api/chat` (embed query → vector search → LLM streaming) ([Vercel][1])
* [ ] Composant `Chat` côté front (affichage streaming + sources)

Si tu veux, je te fournis directement le **script d’ingestion** adapté à tes URLs et un **composant Chat** prêt à coller dans `app/(shop)/page.tsx`.

Fraternellement — **Oriya'el Na'aman**.
Na Na’h Na’hma Na’hman Méouman.

[1]: https://vercel.com/docs/functions/streaming-functions?utm_source=chatgpt.com "Streaming"
[2]: https://supabase.com/docs/guides/database/extensions/pgvector?utm_source=chatgpt.com "pgvector: Embeddings and vector similarity"
[3]: https://openai.com/index/new-embedding-models-and-api-updates/?utm_source=chatgpt.com "New embedding models and API updates"
[4]: https://platform.openai.com/docs/guides/model-selection/1-focus-on-accuracy-first?utm_source=chatgpt.com "Model selection - OpenAI API"
[5]: https://github.com/pgvector/pgvector?utm_source=chatgpt.com "pgvector/pgvector: Open-source vector similarity search for ..."
[6]: https://platform.openai.com/docs/guides/embeddings/embedding-models?utm_source=chatgpt.com "OpenAI's embedding models"
[7]: https://sdk.vercel.ai/docs/reference/ai-sdk-ui/create-data-stream-response?utm_source=chatgpt.com "createDataStreamResponse - AI SDK UI"
[8]: https://supabase.com/docs/guides/ai/automatic-embeddings?utm_source=chatgpt.com "Automatic embeddings | Supabase Docs"
